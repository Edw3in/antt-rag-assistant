from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq # <--- NOVA IMPORTAÃ‡ÃƒO
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

PERSIST_DIR = "./chroma_db"
EMBED_MODEL = "BAAI/bge-m3"
LLM_MODEL = "llama3-8b-8192" # Modelo Llama 3 da Groq
llm = ChatGroq(model=LLM_MODEL, temperature=0.0)
print("ðŸ”„ Carregando sistema RAG...\n")

print("ðŸ“‚ Conectando ao banco de dados...")
emb = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
db = Chroma(persist_directory=PERSIST_DIR, embedding_function=emb)

# Retriever com MMR
retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 8, "fetch_k": 32, "lambda_mult": 0.5}
)

# Prompt especializado
template = """
VocÃª Ã© um assistente tÃ©cnico-jurÃ­dico especializado em concessÃµes rodoviÃ¡rias,
regulamentos da ANTT (como RCR-2, RCR-3, RCR-4, RCR-5) e resoluÃ§Ãµes relacionadas.

Use APENAS as informaÃ§Ãµes presentes nos trechos de "Contexto" para responder.

Regras:
- Responda sempre em portuguÃªs do Brasil, de forma objetiva e tÃ©cnica.
- Quando possÃ­vel, cite explicitamente o nÃºmero da resoluÃ§Ã£o, artigo, parÃ¡grafo
  ou clÃ¡usula contratual (por exemplo: "art. 50 da RCR-3", "ResoluÃ§Ã£o 6.053/2024").
- Se a pergunta for muito genÃ©rica (por exemplo: apenas "seguro"), explique isso
  ao usuÃ¡rio e peÃ§a que detalhe melhor (ex: "seguro de risco de engenharia",
  "seguro de responsabilidade civil - RC-OPER", etc.).
- Se o contexto nÃ£o tiver informaÃ§Ã£o suficiente, diga claramente:
  "Com base apenas nos documentos carregados, nÃ£o encontrei informaÃ§Ã£o suficiente
  para responder com seguranÃ§a."

Contexto:
{context}

Pergunta do usuÃ¡rio:
{question}

Resposta (em portuguÃªs, organizada em tÃ³picos quando fizer sentido):
"""

prompt = PromptTemplate.from_template(template)

qa = RetrievalQA.from_chain_type(
    llm=ChatOllama(model=LLM_MODEL, temperature=0.1),
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
)
db = Chroma(persist_directory=PERSIST_DIR, embedding_function=emb)

# MMR para reduzir redundÃ¢ncia e ampliar cobertura
retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 10, "fetch_k": 40, "lambda_mult": 0.4}
)

print(f"ðŸ¤– Conectando ao modelo {LLM_MODEL} (Ollama)...")
llm = ChatOllama(model=LLM_MODEL, temperature=0.0)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

print("\nâœ… Sistema pronto! FaÃ§a suas perguntas.\n" + "=" * 60)

while True:
    try:
        q = input("\nðŸ’¬ Sua pergunta (ou ENTER para sair): ").strip()
        if not q:
            print("\nðŸ‘‹ AtÃ© logo!")
            break

        print("\nðŸ” Buscando resposta...\n")
        result = qa.invoke({"query": q})

        print("=" * 60)
        print("ðŸ“ RESPOSTA:\n" + result["result"])

        print("\n" + "=" * 60)
        print("ðŸ“š FONTES CONSULTADAS:")
        for i, d in enumerate(result["source_documents"], 1):
            src = d.metadata.get("source", "desconhecido")
            page = d.metadata.get("page", "?")
            print(f"{i}. {src} (pÃ¡gina {page})")
        print("=" * 60)

    except KeyboardInterrupt:
        print("\n\nðŸ‘‹ AtÃ© logo!")
        break
    except Exception as e:
        print(f"\nâŒ Erro: {e}")
